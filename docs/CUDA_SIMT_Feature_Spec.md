# CUDA SIMT ç¼–ç¨‹æ¨¡å‹ Feature Specification
## CUDA-Compatible SIMT Programming Model Development Plan

**ç‰ˆæœ¬:** 1.0  
**æ—¥æœŸ:** 2026-01-31  
**ä½œè€…:** Winston Zhang  
**çŠ¶æ€:** Draft

---

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#1-æ¦‚è¿°)
2. [CUDA SIMT æ ¸å¿ƒç‰¹æ€§åˆ†æ](#2-cuda-simt-æ ¸å¿ƒç‰¹æ€§åˆ†æ)
3. [Feature Specification](#3-feature-specification)
4. [å®ç°æ¶æ„å»ºè®®](#4-å®ç°æ¶æ„å»ºè®®)
5. [å¼€å‘è·¯çº¿å›¾](#5-å¼€å‘è·¯çº¿å›¾)
6. [å…¼å®¹æ€§ç­–ç•¥](#6-å…¼å®¹æ€§ç­–ç•¥)
7. [é£é™©è¯„ä¼°](#7-é£é™©è¯„ä¼°)

---

## 1. æ¦‚è¿°

### 1.1 ç›®æ ‡

åˆ¶å®šä¸€å¥—å®Œå¤‡çš„ã€ä¸ CUDA å…¼å®¹çš„ SIMTï¼ˆSingle Instruction, Multiple Threadsï¼‰ç¼–ç¨‹æ¨¡å‹è§„èŒƒï¼Œç”¨äºæŒ‡å¯¼è‡ªç ” GPU çš„è½¯ä»¶æ ˆå¼€å‘ã€‚

### 1.2 èŒƒå›´

- CUDA Runtime API å…¼å®¹æ€§
- CUDA Driver API å…¼å®¹æ€§
- PTX æŒ‡ä»¤é›†æ¶æ„æ”¯æŒ
- CUDA ç¼–ç¨‹æ¨¡å‹æ ¸å¿ƒæŠ½è±¡ï¼ˆGrid/Block/Threadï¼‰
- å†…å­˜æ¨¡å‹å’Œä¸€è‡´æ€§
- åŒæ­¥åŸè¯­
- æ•°å­¦åº“å’Œ Intrinsic å‡½æ•°

### 1.3 å‚è€ƒæ–‡æ¡£

- NVIDIA CUDA C++ Programming Guide
- PTX ISA Reference Manual
- CUDA Runtime API Documentation
- CUDA Driver API Documentation

---

## 2. CUDA SIMT æ ¸å¿ƒç‰¹æ€§åˆ†æ

### 2.1 æ‰§è¡Œæ¨¡å‹ (Execution Model)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Grid                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      Block 0                     â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    â”‚
â”‚  â”‚  â”‚ Thread0 â”‚ â”‚ Thread1 â”‚ â”‚ Thread2 â”‚ ...       â”‚    â”‚
â”‚  â”‚  â”‚ (Warp0) â”‚ â”‚ (Warp0) â”‚ â”‚ (Warp0) â”‚           â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚    â”‚
â”‚  â”‚  â”‚ Thread32â”‚ â”‚ Thread33â”‚ ...                   â”‚    â”‚
â”‚  â”‚  â”‚ (Warp1) â”‚ â”‚ (Warp1) â”‚                       â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      Block 1                     â”‚    â”‚
â”‚  â”‚                        ...                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å…³é”®æ¦‚å¿µ

| æ¦‚å¿µ | æè¿° | CUDA å¯¹åº” |
|------|------|-----------|
| **Grid** | æ•´ä¸ª GPU å†…æ ¸å¯åŠ¨çš„æ‰€æœ‰çº¿ç¨‹é›†åˆ | `gridDim` |
| **Block** | åä½œçº¿ç¨‹æ•°ç»„ (CTA)ï¼Œå¯åŒæ­¥ã€å…±äº«å†…å­˜ | `blockDim` |
| **Warp** | 32 ä¸ªçº¿ç¨‹ç»„æˆçš„ SIMD æ‰§è¡Œå•å…ƒ | Warp size = 32 |
| **Thread** | åŸºæœ¬æ‰§è¡Œå•å…ƒï¼Œæœ‰ç‹¬ç«‹å¯„å­˜å™¨å’Œç¨‹åºè®¡æ•°å™¨ | `threadIdx` |

### 2.2 å†…å­˜æ¨¡å‹ (Memory Model)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Host Memory                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Global Memory                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚
â”‚  â”‚  â”‚ Block0 â”‚ â”‚ Block1 â”‚ â”‚ Block2 â”‚ â”‚  ...   â”‚        â”‚  â”‚
â”‚  â”‚  â”‚ Shared â”‚ â”‚ Shared â”‚ â”‚ Shared â”‚ â”‚ Shared â”‚        â”‚  â”‚
â”‚  â”‚  â”‚ Memory â”‚ â”‚ Memory â”‚ â”‚ Memory â”‚ â”‚ Memory â”‚        â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚
â”‚  â”‚                                                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚              Constant Memory                    â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â”‚                                                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚              Texture/Surface Memory           â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘                    â†‘
   cudaMemcpy()          cudaMalloc()
```

#### å†…å­˜å±‚çº§

| å†…å­˜ç±»å‹ | ä½œç”¨åŸŸ | ç”Ÿå‘½å‘¨æœŸ | ç¼“å­˜ | è®¿é—®é€Ÿåº¦ |
|----------|--------|----------|------|----------|
| **Register** | Thread | Kernel | - | Fastest |
| **Shared Memory** | Block | Kernel | - | Fast |
| **Global Memory** | Grid | Application | L1/L2 | Slow |
| **Constant Memory** | Grid | Application | Constant Cache | Fast (cached) |
| **Texture Memory** | Grid | Application | Texture Cache | Fast (cached) |
| **Local Memory** | Thread | Kernel | L1/L2 | Slow (spill) |

### 2.3 ç¼–ç¨‹æ¥å£åˆ†å±‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CUDA Libraries (cuBLAS, etc)     â”‚  â† å¯é€‰å®ç°
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         CUDA Runtime API (cudart)        â”‚  â† Phase 2
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         CUDA Driver API (cuda)           â”‚  â† Phase 1
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         PTX Instruction Set              â”‚  â† Core
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         GPU Hardware Abstraction         â”‚  â† Hardware
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Feature Specification

### 3.1 æ‰§è¡Œæ¨¡å‹è§„èŒƒ

#### 3.1.1 çº¿ç¨‹å±‚çº§ç»´åº¦

```c
// å¿…é¡»æ”¯æŒçš„ç»´åº¦æŸ¥è¯¢
__host__ __device__ dim3 gridDim;   // Grid ç»´åº¦ (x, y, z)
__host__ __device__ dim3 blockDim;  // Block ç»´åº¦ (x, y, z)
__host__ __device__ dim3 blockIdx;  // Block ç´¢å¼•
__host__ __device__ dim3 threadIdx; // Thread ç´¢å¼•
__host__ __device__ int warpSize;   // Warp å¤§å° (32)

// é™åˆ¶è¦æ±‚
#define MAX_GRID_DIM_X  2147483647  // 2^31 - 1
#define MAX_GRID_DIM_Y  65535
#define MAX_GRID_DIM_Z  65535
#define MAX_BLOCK_DIM_X 1024
#define MAX_BLOCK_DIM_Y 1024
#define MAX_BLOCK_DIM_Z 64
#define MAX_THREADS_PER_BLOCK 1024
#define WARP_SIZE 32
```

#### 3.1.2 Kernel å¯åŠ¨è¯­æ³•

```c
// åŸºæœ¬è¯­æ³•
__global__ void kernelName(args...);
kernelName<<<gridDim, blockDim, sharedMem, stream>>>(args...);

// å¿…é¡»æ”¯æŒçš„é…ç½®
<<<gridDim, blockDim>>>                    // åŸºæœ¬å¯åŠ¨
<<<gridDim, blockDim, sharedMem>>>         // + å…±äº«å†…å­˜
<<<gridDim, blockDim, sharedMem, stream>>> // + Stream

// åŠ¨æ€å¹¶è¡Œ (CDP) - Phase 3
__global__ void parentKernel() {
    childKernel<<<gridDim, blockDim>>>(args);
}
```

### 3.2 å†…å­˜ç®¡ç†è§„èŒƒ

#### 3.2.1 è®¾å¤‡å†…å­˜åˆ†é…

```c
// Runtime API
cudaError_t cudaMalloc(void** devPtr, size_t size);
cudaError_t cudaMallocHost(void** ptr, size_t size);     // Pinned memory
cudaError_t cudaMallocManaged(void** devPtr, size_t size); // Unified Memory
cudaError_t cudaFree(void* devPtr);
cudaError_t cudaFreeHost(void* ptr);

// Driver API
cuMemAlloc(CUdeviceptr* dptr, size_t bytesize);
cuMemFree(CUdeviceptr dptr);
cuMemAllocHost(void** pp, size_t bytesize);
```

#### 3.2.2 å†…å­˜ä¼ è¾“

```c
// Runtime API
cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, 
                       cudaMemcpyKind kind);
cudaError_t cudaMemcpyAsync(void* dst, const void* src, size_t count,
                            cudaMemcpyKind kind, cudaStream_t stream);
cudaError_t cudaMemcpy2D(void* dst, size_t dpitch, const void* src, 
                         size_t spitch, size_t width, size_t height,
                         cudaMemcpyKind kind);
cudaError_t cudaMemset(void* devPtr, int value, size_t count);

// ä¼ è¾“ç±»å‹
typedef enum cudaMemcpyKind {
    cudaMemcpyHostToHost     = 0,
    cudaMemcpyHostToDevice   = 1,
    cudaMemcpyDeviceToHost   = 2,
    cudaMemcpyDeviceToDevice = 3,
    cudaMemcpyDefault        = 4  // Unified Memory
};
```

#### 3.2.3 å…±äº«å†…å­˜

```c
// é™æ€åˆ†é…
__shared__ float sharedData[256];

// åŠ¨æ€åˆ†é…
extern __shared__ float dynamicShared[];
kernel<<<grid, block, sharedMemSize>>>(args);

// Bank conflict é¿å…
// è¦æ±‚: 32 ä¸ª bankï¼Œæ¯ä¸ª bank 32/64-bit å®½åº¦
// è®¿é—®æ¨¡å¼: stride-1 æ— å†²çªï¼Œstride-2^n (n>=5) æ— å†²çª
```

### 3.3 åŒæ­¥åŸè¯­

#### 3.3.1 Block çº§åˆ«åŒæ­¥

```c
// å¿…é¡»å®ç°
__device__ void __syncthreads(void);
__device__ void __syncthreads_count(int predicate);
__device__ void __syncthreads_and(int predicate);
__device__ void __syncthreads_or(int predicate);

// Warp çº§åˆ«åŒæ­¥ (Compute Capability >= 7.0)
__device__ void __syncwarp(unsigned mask = 0xffffffff);
```

#### 3.3.2 åŸå­æ“ä½œ

```c
// æ•´æ•°åŸå­æ“ä½œ
__device__ int atomicAdd(int* address, int val);
__device__ int atomicSub(int* address, int val);
__device__ int atomicExch(int* address, int val);
__device__ int atomicMin(int* address, int val);
__device__ int atomicMax(int* address, int val);
__device__ int atomicInc(int* address, int val);
__device__ int atomicDec(int* address, int val);
__device__ int atomicCAS(int* address, int compare, int val);
__device__ int atomicAnd(int* address, int val);
__device__ int atomicOr(int* address, int val);
__device__ int atomicXor(int* address, int val);

// æµ®ç‚¹åŸå­æ“ä½œ (CC >= 6.0)
__device__ float atomicAdd(float* address, float val);
__device__ double atomicAdd(double* address, double val);

// 64-bit åŸå­æ“ä½œ
__device__ long long atomicAdd(long long* address, long long val);
```

#### 3.3.3 å†…å­˜å±éšœ

```c
__device__ void __threadfence(void);           // å…¨å±€å†…å­˜å±éšœ
__device__ void __threadfence_block(void);     // Block çº§åˆ«å±éšœ
__device__ void __threadfence_system(void);    // ç³»ç»Ÿçº§åˆ«å±éšœ (CC >= 2.0)
```

### 3.4 PTX æŒ‡ä»¤é›†æ”¯æŒ

#### 3.4.1 æ ¸å¿ƒæŒ‡ä»¤ç±»åˆ«

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PTX Instruction Classes                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Memory Access Instructions                               â”‚
â”‚    - ld, st, mov, cvta, isspacep                           â”‚
â”‚    - Special: ld.global.nc (cache streaming)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2. Integer Arithmetic                                       â”‚
â”‚    - add, sub, mul, mad, div, rem                          â”‚
â”‚    - abs, neg, min, max                                    â”‚
â”‚    - shl, shr, and, or, xor, not, cnot, popc, clz, bfind  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 3. Floating-Point Arithmetic                                â”‚
â”‚    - add, sub, mul, fma, div, rem, sqrt, rsqrt             â”‚
â”‚    - abs, neg, min, max, saturating ops                    â”‚
â”‚    - sin, cos, lg2, ex2 (SFU)                              â”‚
â”‚    - Special: tensor core MMA (WMMA)                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4. Comparison and Selection                                 â”‚
â”‚    - setp, selp, slct                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 5. Data Movement and Conversion                             â”‚
â”‚    - mov, cvta, cvt, prmt                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 6. Control Flow Instructions                                â”‚
â”‚    - bra, call, ret, exit, @%pred bra                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 7. Parallel Synchronization and Communication               â”‚
â”‚    - bar, membar, atom, red, vote                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 8. Texture Instructions                                     â”‚
â”‚    - tex, tld4, txq                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 9. Surface Instructions                                     â”‚
â”‚    - suLd, suSt, suatom                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.4.2 Warp Shuffle Instructions

```c
// Warp çº§åˆ«æ•°æ®äº¤æ¢ (CC >= 3.0)
__device__ int __shfl_sync(unsigned mask, int var, int srcLane, int width=warpSize);
__device__ int __shfl_up_sync(unsigned mask, int var, unsigned int delta, int width=warpSize);
__device__ int __shfl_down_sync(unsigned mask, int var, unsigned int delta, int width=warpSize);
__device__ int __shfl_xor_sync(unsigned mask, int var, int laneMask, int width=warpSize);

// ç¤ºä¾‹: Warp è§„çº¦
__inline__ __device__ int warpReduceSum(int val) {
    for (int offset = warpSize/2; offset > 0; offset /= 2)
        val += __shfl_down_sync(0xFFFFFFFF, val, offset);
    return val;
}
```

#### 3.4.3 Cooperative Groups (CC >= 6.0)

```c
// çº¿ç¨‹ç»„åŸè¯­
#include <cooperative_groups.h>
namespace cg = cooperative_groups;

// åŸºæœ¬ç»„æ“ä½œ
cg::thread_group g = cg::this_thread_block();
int size = g.size();
int rank = g.thread_rank();
g.sync();

// çº¿ç¨‹å—ç»„
cg::thread_block block = cg::this_thread_block();
cg::thread_block_tile<32> tile32 = cg::tiled_partition<32>(block);

// å¤šçº¿ç¨‹å—ç»„ (CC >= 9.0)
cg::grid_group grid = cg::this_grid();
grid.sync();  // å…¨å±€åŒæ­¥
```

### 3.5 æ•°å­¦åº“æ”¯æŒ

#### 3.5.1 æ ‡å‡†æ•°å­¦å‡½æ•°

```c
// å¿…é¡»æ”¯æŒçš„æ•°å­¦å‡½æ•°
#include <math.h>

// ä¸‰è§’å‡½æ•°
__device__ float sinf(float x);
__device__ float cosf(float x);
__device__ float tanf(float x);
__device__ float sinpif(float x);   // Ï€-scale
__device__ float cospif(float x);

// æŒ‡æ•°å’Œå¯¹æ•°
__device__ float expf(float x);
__device__ float exp2f(float x);
__device__ float exp10f(float x);
__device__ float logf(float x);
__device__ float log2f(float x);
__device__ float log10f(float x);

// å¹‚å‡½æ•°
__device__ float powf(float x, float y);
__device__ float sqrtf(float x);
__device__ float rsqrtf(float x);    // 1/sqrt
__device__ float cbrtf(float x);     // cube root

// å…¶ä»–
__device__ float ceilf(float x);
__device__ float floorf(float x);
__device__ float truncf(float x);
__device__ float roundf(float x);
__device__ float fabsf(float x);
__device__ float fminf(float x, float y);
__device__ float fmaxf(float x, float y);

// å†…åœ¨å‡½æ•° (Intrinsic) - æ›´å¿«ä½†ç²¾åº¦è¾ƒä½
__device__ float __sinf(float x);
__device__ float __cosf(float x);
__device__ float __expf(float x);
__device__ float __logf(float x);
```

#### 3.5.2 åŠç²¾åº¦æµ®ç‚¹ (FP16)

```c
#include <cuda_fp16.h>

// ç±»å‹
__half, __half2, __half_raw;

// è½¬æ¢
__device__ __half __float2half_rn(float f);
__device__ float __half2float(__half h);
__device__ __half2 __floats2half2_rn(float f1, float f2);
__device__ float2 __half22float2(__half2 h2);

// ç®—æœ¯è¿ç®—
__device__ __half __hadd(__half a, __half b);
__device__ __half __hsub(__half a, __half b);
__device__ __half __hmul(__half a, __half b);
__device__ __half __hfma(__half a, __half b, __half c);

// Vector æ“ä½œ
__device__ __half2 __hadd2(__half2 a, __half2 b);
__device__ __half2 __hmul2(__half2 a, __half2 b);
```

### 3.6 Stream å’Œ Event

```c
// Stream ç®¡ç†
cudaError_t cudaStreamCreate(cudaStream_t* pStream);
cudaError_t cudaStreamCreateWithFlags(cudaStream_t* pStream, unsigned int flags);
cudaError_t cudaStreamDestroy(cudaStream_t stream);
cudaError_t cudaStreamSynchronize(cudaStream_t stream);

// Stream æ ‡å¿—
#define cudaStreamDefault      0x00
#define cudaStreamNonBlocking  0x01

// Event ç®¡ç†
cudaError_t cudaEventCreate(cudaEvent_t* event);
cudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, unsigned int flags);
cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream);
cudaError_t cudaEventSynchronize(cudaEvent_t event);
cudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t stop);
cudaError_t cudaEventDestroy(cudaEvent_t event);

// Event æ ‡å¿—
#define cudaEventDefault        0x00
#define cudaEventBlockingSync   0x01
#define cudaEventDisableTiming  0x02
```

### 3.7 ç»Ÿä¸€å†…å­˜ (Unified Memory)

```c
// ç³»ç»Ÿåˆ†é…å™¨
cudaError_t cudaMallocManaged(void** devPtr, size_t size, unsigned int flags = cudaMemAttachGlobal);
cudaError_t cudaFree(void* devPtr);

// Prefetch æç¤º (CC >= 6.0)
cudaError_t cudaMemPrefetchAsync(const void* devPtr, size_t count, int dstDevice,
                                 cudaStream_t stream);

// è®¿é—®å»ºè®®
cudaError_t cudaMemAdvise(const void* devPtr, size_t count, cudaMemoryAdvise advice, int device);

// Advice ç±»å‹
typedef enum cudaMemoryAdvise {
    cudaMemAdviseSetReadMostly          = 1,
    cudaMemAdviseUnsetReadMostly        = 2,
    cudaMemAdviseSetPreferredLocation   = 3,
    cudaMemAdviseUnsetPreferredLocation = 4,
    cudaMemAdviseSetAccessedBy          = 5,
    cudaMemAdviseUnsetAccessedBy        = 6,
    cudaMemAdviseSetReadMostlyCuda     = 1  // deprecated
};
```

---

## 4. å®ç°æ¶æ„å»ºè®®

### 4.1 è½¯ä»¶æ ˆæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Application Layer                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CUDA Libraries (cuBLAS, cuDNN, cuFFT, NCCL, Thrust, etc.)     â”‚  â† Phase 4
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CUDA Runtime (libcudart.so)                                    â”‚  â† Phase 3
â”‚  - Memory management                                            â”‚
â”‚  - Kernel launch                                                â”‚
â”‚  - Stream/Event                                                 â”‚
â”‚  - Error handling                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CUDA Driver (libcuda.so)                                       â”‚  â† Phase 2
â”‚  - Context management                                           â”‚
â”‚  - Module loading                                               â”‚
â”‚  - Memory allocation                                            â”‚
â”‚  - Execution control                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PTX JIT Compiler                                               â”‚  â† Phase 2
â”‚  - PTX â†’ è‡ªç ” ISA                                               â”‚
â”‚  - ä¼˜åŒ– passes                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Runtime (è‡ªç ”)                                                  â”‚  â† Phase 1
â”‚  - Command submission                                           â”‚
â”‚  - Memory management                                            â”‚
â”‚  - Queue/Scheduler                                              â”‚
â”‚  - Interrupt handling                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kernel Driver (è‡ªç ”)                                            â”‚  â† Phase 1
â”‚  - Hardware abstraction                                         â”‚
â”‚  - Memory mapping                                               â”‚
â”‚  - Context switch                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 å…³é”®ç»„ä»¶è®¾è®¡

#### 4.2.1 PTX ç¿»è¯‘å±‚

```c
// PTX ç¿»è¯‘å™¨æ¶æ„
class PTXTranslator {
public:
    // PTX è§£æ
    std::unique_ptr<PTXModule> parse(const char* ptxCode);
    
    // ISA ç”Ÿæˆ
    std::vector<Instruction> generateISA(const PTXModule& module);
    
    // ä¼˜åŒ– passes
    void runOptimizationPasses(std::vector<Instruction>& isa);
    
private:
    // æŒ‡ä»¤æ˜ å°„è¡¨
    std::unordered_map<std::string, InstructionMapping> instMap_;
    
    // å¯„å­˜å™¨åˆ†é…
    RegisterAllocator regAlloc_;
    
    // Barrier/åŒæ­¥å¤„ç†
    SyncPatternAnalyzer syncAnalyzer_;
};

// å…³é”®å®ç°ç‚¹
// 1. PTX æŒ‡ä»¤ â†’ è‡ªç ” ISA æ˜ å°„
// 2. 32-thread Warp æ¨¡æ‹Ÿ
// 3. åˆ†æ”¯åˆ†æ­§ (Branch Divergence) å¤„ç†
// 4. å†…å­˜è®¿é—®æ¨¡å¼ä¼˜åŒ–
```

#### 4.2.2 Warp è°ƒåº¦å™¨

```c
// Warp è°ƒåº¦å™¨è®¾è®¡
class WarpScheduler {
public:
    // Warp çŠ¶æ€
    enum class WarpState {
        ACTIVE,      // æ­£åœ¨æ‰§è¡Œ
        BARRIER,     // ç­‰å¾…åŒæ­¥
        MEMORY,      // ç­‰å¾…å†…å­˜
        DIVERGED,    // åˆ†æ”¯åˆ†æ­§
        FINISHED     // å®Œæˆ
    };
    
    // è°ƒåº¦ç­–ç•¥
    enum class SchedulePolicy {
        ROUND_ROBIN,     // è½®è¯¢
        GREEDY,          // è´ªå©ª (ä¼˜å…ˆ ready Warp)
        TWO_LEVEL        // ä¸¤çº§è°ƒåº¦
    };
    
    void scheduleWarp();
    void handleDivergence(Warp& warp, BranchInstr* branch);
    void reconvergeWarp(Warp& warp);
    
private:
    std::vector<Warp> warps_;
    SchedulePolicy policy_;
    int maxActiveWarps_;
};
```

#### 4.2.3 å†…å­˜å­ç³»ç»Ÿ

```c
// å†…å­˜å±‚æ¬¡ç»“æ„
class MemorySubsystem {
public:
    // å…¨å±€å†…å­˜è®¿é—®
    void globalLoad(uint64_t addr, void* data, size_t size);
    void globalStore(uint64_t addr, const void* data, size_t size);
    
    // å…±äº«å†…å­˜è®¿é—®
    void sharedLoad(uint32_t smemAddr, void* data, size_t size);
    void sharedStore(uint32_t smemAddr, const void* data, size_t size);
    
    // Cache ç®¡ç†
    void invalidateL1();
    void flushL2();
    
    // ä¸€è‡´æ€§ä¿è¯
    void memoryFence(MemoryScope scope);
    
private:
    L1Cache l1Cache_;
    L2Cache l2Cache_;
    SharedMemory sharedMem_;
    GlobalMemory globalMem_;
};

// Bank conflict æ£€æµ‹
bool hasBankConflict(const std::vector<uint32_t>& addresses) {
    std::unordered_set<uint32_t> banks;
    for (auto addr : addresses) {
        uint32_t bank = (addr / 4) % 32;  // 32 banks, 4 bytes width
        if (banks.count(bank)) return true;
        banks.insert(bank);
    }
    return false;
}
```

### 4.3 ç¡¬ä»¶æŠ½è±¡å±‚

```c
// ç¡¬ä»¶èƒ½åŠ›æŸ¥è¯¢
struct DeviceCapabilities {
    int computeCapabilityMajor;     // è®¡ç®—èƒ½åŠ›ä¸»ç‰ˆæœ¬
    int computeCapabilityMinor;     // è®¡ç®—èƒ½åŠ›æ¬¡ç‰ˆæœ¬
    int maxThreadsPerBlock;         // 1024
    int maxBlockDimX, maxBlockDimY, maxBlockDimZ;
    int maxGridDimX, maxGridDimY, maxGridDimZ;
    int maxSharedMemoryPerBlock;    // 48KB (CC < 7.0) / 96KB (CC >= 7.0)
    int maxRegistersPerBlock;       // 64K
    int warpSize;                   // 32
    int multiProcessorCount;
    size_t totalGlobalMem;
    int maxTexture1D;
    int maxTexture2D[2];
    int maxTexture3D[3];
    int maxClockRate;
    int memoryClockRate;
    int memoryBusWidth;
};

// ç¡¬ä»¶æŠ½è±¡æ¥å£
class HardwareAbstraction {
public:
    virtual void queryCapabilities(DeviceCapabilities& caps) = 0;
    virtual void* allocateDeviceMemory(size_t size) = 0;
    virtual void freeDeviceMemory(void* ptr) = 0;
    virtual void copyToDevice(void* dst, const void* src, size_t size) = 0;
    virtual void copyToHost(void* dst, const void* src, size_t size) = 0;
    virtual void launchKernel(const KernelConfig& config, const void* args) = 0;
    virtual void synchronize() = 0;
};
```

---

## 5. å¼€å‘è·¯çº¿å›¾

### 5.1 Phase 1: åŸºç¡€ Runtime (6ä¸ªæœˆ)

**ç›®æ ‡:** å®ç°æœ€å°å¯è¿è¡Œ CUDA ç¨‹åº

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å·¥æ—¶ | äº¤ä»˜ç‰© |
|------|--------|------|--------|
| Kernel Driver å¼€å‘ | P0 | 8å‘¨ | å†…æ ¸é©±åŠ¨æ¨¡å— |
| Runtime æ ¸å¿ƒæ¡†æ¶ | P0 | 6å‘¨ | libcuda_runtime.so |
| åŸºæœ¬å†…å­˜ç®¡ç† | P0 | 4å‘¨ | cudaMalloc/cudaFree |
| åŸºç¡€ PTX ç¿»è¯‘å™¨ | P0 | 8å‘¨ | PTX â†’ è‡ªç ” ISA |
| ç®€å• Kernel å¯åŠ¨ | P0 | 4å‘¨ | <<< >>> è¯­æ³•æ”¯æŒ |
| Warp è°ƒåº¦å™¨ | P1 | 6å‘¨ | åŸºç¡€è°ƒåº¦å®ç° |
| æµ‹è¯•æ¡†æ¶ | P1 | 4å‘¨ | å•å…ƒæµ‹è¯• + é›†æˆæµ‹è¯• |

**Phase 1 éªŒæ”¶æ ‡å‡†:**
- [ ] èƒ½å¤Ÿç¼–è¯‘å¹¶è¿è¡Œç®€å•çš„ vectorAdd CUDA ç¨‹åº
- [ ] æ”¯æŒåŸºæœ¬çš„ threadIdx/blockIdx æŸ¥è¯¢
- [ ] æ”¯æŒå…¨å±€å†…å­˜è¯»å†™
- [ ] èƒ½å¤Ÿé€šè¿‡ CUDA Samples ä¸­çš„ simpleAssert æµ‹è¯•

### 5.2 Phase 2: Driver API å®Œæ•´æ”¯æŒ (4ä¸ªæœˆ)

**ç›®æ ‡:** å®ç° CUDA Driver API å®Œæ•´åŠŸèƒ½

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å·¥æ—¶ | äº¤ä»˜ç‰© |
|------|--------|------|--------|
| Context ç®¡ç† | P0 | 3å‘¨ | cuCtxCreate/cuCtxDestroy |
| Module åŠ è½½ | P0 | 4å‘¨ | cuModuleLoad/cuModuleGetFunction |
| å®Œæ•´ PTX æ”¯æŒ | P0 | 8å‘¨ | 95%+ PTX æŒ‡ä»¤è¦†ç›–ç‡ |
| Stream ç®¡ç† | P1 | 3å‘¨ | cuStreamCreate/cuStreamSynchronize |
| Event ç®¡ç† | P1 | 2å‘¨ | cuEventRecord/cuEventElapsedTime |
| çº¹ç†å†…å­˜ | P2 | 4å‘¨ | åŸºç¡€çº¹ç†æ”¯æŒ |
| æ€§èƒ½åˆ†æå·¥å…· | P2 | 3å‘¨ | nvprof å…¼å®¹æ¥å£ |

**Phase 2 éªŒæ”¶æ ‡å‡†:**
- [ ] æ”¯æŒ CUDA Samples ä¸­ 80% çš„æµ‹è¯•ç”¨ä¾‹
- [ ] èƒ½å¤Ÿé€šè¿‡ cuBLAS åŸºç¡€æµ‹è¯•
- [ ] æ€§èƒ½è¾¾åˆ° NVIDIA GPU çš„ 60%+

### 5.3 Phase 3: Runtime API å®Œæ•´æ”¯æŒ (4ä¸ªæœˆ)

**ç›®æ ‡:** å®ç° CUDA Runtime API å®Œæ•´åŠŸèƒ½

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å·¥æ—¶ | äº¤ä»˜ç‰© |
|------|--------|------|--------|
| Runtime API å°è£… | P0 | 6å‘¨ | libcudart.so |
| é”™è¯¯å¤„ç† | P0 | 2å‘¨ | cudaGetLastError ç­‰ |
| è®¾å¤‡ç®¡ç† | P1 | 3å‘¨ | cudaGetDevice/cudaSetDevice |
| å†…å­˜æ± ä¼˜åŒ– | P1 | 4å‘¨ | cudaMallocAsync (CC >= 11.2) |
| ç»Ÿä¸€å†…å­˜ | P1 | 4å‘¨ | cudaMallocManaged |
| Graph æ”¯æŒ | P2 | 4å‘¨ | CUDA Graph (CC >= 10.0) |
| å¤šè®¾å¤‡æ”¯æŒ | P2 | 3å‘¨ | Peer-to-peer è®¿é—® |

**Phase 3 éªŒæ”¶æ ‡å‡†:**
- [ ] æ”¯æŒ PyTorch åŸºç¡€è¿è¡Œ
- [ ] æ”¯æŒ TensorFlow åŸºç¡€è¿è¡Œ
- [ ] é€šè¿‡ CUDA Samples 95% æµ‹è¯•

### 5.4 Phase 4: åº“å…¼å®¹ä¸ä¼˜åŒ– (6ä¸ªæœˆ)

**ç›®æ ‡:** å®ç°ä¸»æµ CUDA åº“å…¼å®¹

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å·¥æ—¶ | äº¤ä»˜ç‰© |
|------|--------|------|--------|
| cuBLAS å…¼å®¹å±‚ | P0 | 8å‘¨ | åŸºç¡€ BLAS åŠŸèƒ½ |
| cuDNN å…¼å®¹å±‚ | P0 | 10å‘¨ | åŸºç¡€æ·±åº¦å­¦ä¹ ç®—å­ |
| cuFFT å…¼å®¹å±‚ | P1 | 6å‘¨ | FFT æ”¯æŒ |
| NCCL å…¼å®¹å±‚ | P1 | 6å‘¨ | å¤šå¡é€šä¿¡ |
| Thrust æ”¯æŒ | P2 | 4å‘¨ | æ ‡å‡†ç®—æ³•åº“ |
| CUTLASS é›†æˆ | P2 | 6å‘¨ | é«˜æ€§èƒ½ GEMM |
| æ€§èƒ½ä¼˜åŒ– | P0 | æŒç»­ | è¾¾åˆ° NVIDIA 80%+ |

**Phase 4 éªŒæ”¶æ ‡å‡†:**
- [ ] è¿è¡Œ ResNet-50 è®­ç»ƒ
- [ ] è¿è¡Œ BERT æ¨ç†
- [ ] æ€§èƒ½è¾¾åˆ° NVIDIA A100 çš„ 70%+

### 5.5 é‡Œç¨‹ç¢‘æ—¶é—´çº¿

```
Month:  1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20
        |â†â”€â”€â”€â”€ Phase 1 â”€â”€â”€â”€â†’|â†â”€â”€â”€â”€ Phase 2 â”€â”€â”€â”€â†’|â†â”€â”€â”€â”€ Phase 3 â”€â”€â”€â”€â†’|â†â”€â”€â”€â”€â”€â”€ Phase 4 â”€â”€â”€â”€â”€â”€â†’|
        
M1:     â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
M2:     â–‘â–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
M3:     â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
M4:     â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘

äº¤ä»˜ç‰©:
â”œâ”€â”€ M3: åŸºç¡€ Runtime Demo (vectorAdd è¿è¡Œ)
â”œâ”€â”€ M6: Phase 1 å®Œæˆ
â”œâ”€â”€ M10: Phase 2 å®Œæˆ
â”œâ”€â”€ M14: Phase 3 å®Œæˆ
â”œâ”€â”€ M18: Phase 4 å®Œæˆ
â””â”€â”€ M20: æ­£å¼å‘å¸ƒ v1.0
```

---

## 6. å…¼å®¹æ€§ç­–ç•¥

### 6.1 CUDA ç‰ˆæœ¬å…¼å®¹æ€§

```
CUDA Version Support Matrix
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature         â”‚ CUDA 10.2â”‚ CUDA 11.8â”‚ CUDA 12.xâ”‚ Priority â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Runtime API     â”‚    âœ“     â”‚    âœ“     â”‚    âœ“     â”‚   P0     â”‚
â”‚ Driver API      â”‚    âœ“     â”‚    âœ“     â”‚    âœ“     â”‚   P0     â”‚
â”‚ PTX ISA 6.x     â”‚    âœ“     â”‚    âœ“     â”‚    âœ“     â”‚   P0     â”‚
â”‚ PTX ISA 7.x     â”‚    -     â”‚    âœ“     â”‚    âœ“     â”‚   P1     â”‚
â”‚ PTX ISA 8.x     â”‚    -     â”‚    -     â”‚    âœ“     â”‚   P2     â”‚
â”‚ CUDA Graph      â”‚    -     â”‚    âœ“     â”‚    âœ“     â”‚   P1     â”‚
â”‚ Unified Memory  â”‚    âœ“     â”‚    âœ“     â”‚    âœ“     â”‚   P1     â”‚
â”‚ Stream Ordered  â”‚    -     â”‚    âœ“     â”‚    âœ“     â”‚   P2     â”‚
â”‚ Async Allocator â”‚          â”‚          â”‚          â”‚          â”‚
â”‚ FP8/TF32        â”‚    -     â”‚    âœ“     â”‚    âœ“     â”‚   P2     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 åº”ç”¨å…¼å®¹æ€§æµ‹è¯•çŸ©é˜µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application         â”‚ Min Versionâ”‚ Target     â”‚ Test Status â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PyTorch             â”‚ 1.9.0      â”‚ 2.0+       â”‚ Phase 3     â”‚
â”‚ TensorFlow          â”‚ 2.8.0      â”‚ 2.13+      â”‚ Phase 3     â”‚
â”‚ ONNX Runtime        â”‚ 1.12       â”‚ 1.15+      â”‚ Phase 4     â”‚
â”‚ TensorRT            â”‚ 8.4        â”‚ 8.6+       â”‚ Phase 4     â”‚
â”‚ CUDA Samples        â”‚ 11.0       â”‚ 12.0+      â”‚ Phase 1-3   â”‚
â”‚ Rodinia Benchmark   â”‚ 3.1        â”‚ 3.1        â”‚ Phase 2     â”‚
â”‚ SHOC Benchmark      â”‚ 1.1.5      â”‚ 1.1.5      â”‚ Phase 2     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 PTX å…¼å®¹æ€§åˆ†çº§

```c
// PTX æŒ‡ä»¤å®ç°ä¼˜å…ˆçº§

// Tier 1: å¿…é¡»å®ç° (Phase 1-2)
// åŸºæœ¬ç®—æœ¯ã€é€»è¾‘ã€æ§åˆ¶æµ
#define TIER1_INSTRUCTIONS 100  // 100% coverage required

// Tier 2: é‡è¦åŠŸèƒ½ (Phase 2-3)
// çº¹ç†ã€è¡¨é¢ã€åŸå­æ“ä½œã€åŒæ­¥
#define TIER2_INSTRUCTIONS 95   // 95% coverage required

// Tier 3: é«˜çº§ç‰¹æ€§ (Phase 3-4)
// Tensor Coreã€åä½œç»„ã€å†…è”æ±‡ç¼–
#define TIER3_INSTRUCTIONS 80   // 80% coverage required

// Tier 4: å¯é€‰ç‰¹æ€§ (Phase 4+)
// è°ƒè¯•ç¬¦å·ã€ç‰¹æ®Šä¼˜åŒ–æŒ‡ä»¤
#define TIER4_INSTRUCTIONS 50   // 50% coverage acceptable
```

---

## 7. é£é™©è¯„ä¼°

### 7.1 æŠ€æœ¯é£é™©

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½ |
|------|--------|------|----------|
| PTX ç¿»è¯‘å¤æ‚åº¦é«˜ | é«˜ | é«˜ | 1. é‡‡ç”¨åˆ†å±‚ç¿»è¯‘æ¶æ„<br>2. é€æ­¥å¢åŠ æŒ‡ä»¤æ”¯æŒ<br>3. å»ºç«‹å®Œå–„çš„æµ‹è¯•è¦†ç›– |
| æ€§èƒ½ä¸è¾¾é¢„æœŸ | ä¸­ | é«˜ | 1. æ—©æœŸè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•<br>2. é¢„ç•™ä¼˜åŒ–ç©ºé—´<br>3. å¯¹æ ‡ NVIDIA æ¶æ„ä¼˜åŒ– |
| å†…å­˜æ¨¡å‹å·®å¼‚ | ä¸­ | ä¸­ | 1. è¯¦ç»†æ–‡æ¡£åŒ–å†…å­˜è¡Œä¸º<br>2. æä¾›å†…å­˜ä¸€è‡´æ€§è°ƒè¯•å·¥å…·<br>3. å»ºç«‹æµ‹è¯•ç”¨ä¾‹ |
| Warp è°ƒåº¦å·®å¼‚ | ä¸­ | ä¸­ | 1. æ¨¡æ‹Ÿ NVIDIA Warp è¡Œä¸º<br>2. æä¾›è°ƒåº¦ç­–ç•¥é…ç½®<br>3. å……åˆ†çš„å¹¶å‘æµ‹è¯• |

### 7.2 é¡¹ç›®ç®¡ç†é£é™©

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½ |
|------|--------|------|----------|
| è¿›åº¦å»¶æœŸ | ä¸­ | é«˜ | 1. è®¾ç½®ç¼“å†²æ—¶é—´<br>2. åˆ†é˜¶æ®µäº¤ä»˜<br>3. æ ¸å¿ƒåŠŸèƒ½ä¼˜å…ˆ |
| äººå‘˜æµåŠ¨ | ä½ | ä¸­ | 1. æ–‡æ¡£åŒ–å…³é”®è®¾è®¡<br>2. ä»£ç å®¡æŸ¥æœºåˆ¶<br>3. çŸ¥è¯†åˆ†äº«ä¼šè®® |
| éœ€æ±‚å˜æ›´ | ä¸­ | ä¸­ | 1. å»ºç«‹å˜æ›´æ§åˆ¶æµç¨‹<br>2. æ•æ·å¼€å‘æ–¹æ³•<br>3. å®šæœŸè¯„å®¡ä¼šè®® |

### 7.3 åˆè§„é£é™©

| é£é™© | å¯èƒ½æ€§ | å½±å“ | ç¼“è§£æªæ–½ |
|------|--------|------|----------|
| CUDA ä¸“åˆ©é—®é¢˜ | ä½ | é«˜ | 1. æ³•å¾‹é¡¾é—®å®¡æŸ¥<br>2. æ¸…æ´å®¤è®¾è®¡æ–¹æ³•<br>3. å…³æ³¨å¼€æºå®ç° |
| å‡ºå£ç®¡åˆ¶ | ä½ | é«˜ | 1. åˆè§„å›¢é˜Ÿå‚ä¸<br>2. äº†è§£ç›¸å…³æ³•è§„<br>3. é¿å…ä½¿ç”¨å—é™æŠ€æœ¯ |

---

## 8. é™„å½•

### 8.1 å‚è€ƒèµ„æº

- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [PTX ISA Reference](https://docs.nvidia.com/cuda/parallel-thread-execution/)
- [CUDA Runtime API](https://docs.nvidia.com/cuda/cuda-runtime-api/)
- [CUDA Driver API](https://docs.nvidia.com/cuda/cuda-driver-api/)
- [LLVM PTX Backend](https://llvm.org/docs/NVPTXUsage.html)

### 8.2 ç›¸å…³å¼€æºé¡¹ç›®

| é¡¹ç›® | æè¿° | è®¸å¯ |
|------|------|------|
| [GPUOcelot](https://github.com/gtcasl/gpuocelot) | NVIDIA PTX æ‰§è¡Œæ¨¡æ‹Ÿå™¨ | BSD |
| [Coriander](https://github.com/hughperkins/coriander) | CUDA â†’ OpenCL è½¬æ¢ | MIT |
| [cupti](https://developer.nvidia.com/cupti) | CUDA Profiling å·¥å…· | ä¸“æœ‰ |
| [Triton](https://github.com/openai/triton) | Python DSL for GPU | MIT |

### 8.3 æœ¯è¯­è¡¨

| æœ¯è¯­ | è¯´æ˜ |
|------|------|
| **SIMT** | Single Instruction, Multiple Thread - å•æŒ‡ä»¤å¤šçº¿ç¨‹ |
| **PTX** | Parallel Thread Execution - å¹¶è¡Œçº¿ç¨‹æ‰§è¡ŒæŒ‡ä»¤é›† |
| **CTA** | Cooperative Thread Array - åä½œçº¿ç¨‹æ•°ç»„ (å³ Block) |
| **Warp** | 32 ä¸ªçº¿ç¨‹ç»„æˆçš„ SIMD æ‰§è¡Œå•å…ƒ |
| **Kernel** | åœ¨ GPU ä¸Šæ‰§è¡Œçš„å‡½æ•° |
| **Grid** | æ‰§è¡ŒåŒä¸€ Kernel çš„æ‰€æœ‰ Block é›†åˆ |
| **Shared Memory** | Block çº§åˆ«çš„å¿«é€Ÿå…±äº«å†…å­˜ |
| **Global Memory** | GPU å…¨å±€å†…å­˜ |
| **Unified Memory** | ç»Ÿä¸€å¯»å€ç©ºé—´ (CPU/GPU å…±äº«) |
| **Bank Conflict** | å…±äº«å†…å­˜ Bank å†²çª |
| **Branch Divergence** | Warp å†…çº¿ç¨‹åˆ†æ”¯åˆ†æ­§ |
| **Cooperative Groups** | CUDA åä½œç»„çº¿ç¨‹åŒæ­¥æœºåˆ¶ |
| **Tensor Core** | NVIDIA Tensor è®¡ç®—å•å…ƒ |
| **Occupancy** | SM å ç”¨ç‡ |

---

## 9. å®¡æ‰¹è®°å½•

| ç‰ˆæœ¬ | æ—¥æœŸ | ä½œè€… | å˜æ›´æè¿° | å®¡æ‰¹äºº |
|------|------|------|----------|--------|
| 1.0 | 2026-01-31 | Winston Zhang | åˆå§‹ç‰ˆæœ¬ | TBD |

---

**æ–‡æ¡£çŠ¶æ€:** Draft  
**ä¸‹æ¬¡è¯„å®¡:** 2026-02-15  
**æ–‡æ¡£æ‰€æœ‰è€…:** Winston Zhang
